# Description

This notebook contains the code and explanation for the Python implementation of predicting **customer churn** and identifying the **factors** that contribute to the churning of a customer for a telecommunications company.

The dataset is that of a telecommunications company who have provided several details about the plan and the features subscribed by the customer. The column _Churn_ indicates whether the customer has churned in the past three months or not. Our objective is to build a predictive model that can accurately predict the churn of customers.

For a better understanding of the dataset, refer [here](https://www.kaggle.com/blastchar/telco-customer-churn).

The contents of the notebook is given below. The notebook can be found [here]().

<h2>Table of Contents<span class="tocSkip"></span></h2>
<div class="toc"><ul class="toc-item"><li><span><a href="#Load,-Inspect-&amp;-Clean" data-toc-modified-id="Load,-Inspect-&amp;-Clean-1"><span class="toc-item-num">1&nbsp;&nbsp;</span>Load, Inspect &amp; Clean</a></span></li><li><span><a href="#Exploratory-Data-Analysis" data-toc-modified-id="Exploratory-Data-Analysis-2"><span class="toc-item-num">2&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class="toc-item"><li><span><a href="#Feature-Analysis" data-toc-modified-id="Feature-Analysis-2.1"><span class="toc-item-num">2.1&nbsp;&nbsp;</span>Feature Analysis</a></span></li></ul></li><li><span><a href="#Feature-Engineering" data-toc-modified-id="Feature-Engineering-3"><span class="toc-item-num">3&nbsp;&nbsp;</span>Feature Engineering</a></span></li><li><span><a href="#Feature-Preprocessing" data-toc-modified-id="Feature-Preprocessing-4"><span class="toc-item-num">4&nbsp;&nbsp;</span>Feature Preprocessing</a></span></li><li><span><a href="#Modelling" data-toc-modified-id="Modelling-5"><span class="toc-item-num">5&nbsp;&nbsp;</span>Modelling</a></span><ul class="toc-item"><li><span><a href="#Fitting-Logistic-Regression" data-toc-modified-id="Fitting-Logistic-Regression-5.1"><span class="toc-item-num">5.1&nbsp;&nbsp;</span>Fitting Logistic Regression</a></span><ul class="toc-item"><li><span><a href="#Feature-Importance" data-toc-modified-id="Feature-Importance-5.1.1"><span class="toc-item-num">5.1.1&nbsp;&nbsp;</span>Feature Importance</a></span></li></ul></li><li><span><a href="#Fitting-Decision-Tree" data-toc-modified-id="Fitting-Decision-Tree-5.2"><span class="toc-item-num">5.2&nbsp;&nbsp;</span>Fitting Decision Tree</a></span><ul class="toc-item"><li><span><a href="#Tuning-Decision-Tree" data-toc-modified-id="Tuning-Decision-Tree-5.2.1"><span class="toc-item-num">5.2.1&nbsp;&nbsp;</span>Tuning Decision Tree</a></span></li><li><span><a href="#Decision-Tree-Autotune" data-toc-modified-id="Decision-Tree-Autotune-5.2.2"><span class="toc-item-num">5.2.2&nbsp;&nbsp;</span>Decision Tree Autotune</a></span></li></ul></li><li><span><a href="#Fitting-Random-Forest" data-toc-modified-id="Fitting-Random-Forest-5.3"><span class="toc-item-num">5.3&nbsp;&nbsp;</span>Fitting Random Forest</a></span></li></ul></li><li><span><a href="#Feature-Selection" data-toc-modified-id="Feature-Selection-6"><span class="toc-item-num">6&nbsp;&nbsp;</span>Feature Selection</a></span><ul class="toc-item"><li><span><a href="#Dropping-highly-cormrelated-features" data-toc-modified-id="Dropping-highly-cormrelated-features-6.1"><span class="toc-item-num">6.1&nbsp;&nbsp;</span>Dropping highly cormrelated features</a></span></li><li><span><a href="#Evaluating-model-using-Recursive-Feature-Elimination-(RFE)" data-toc-modified-id="Evaluating-model-using-Recursive-Feature-Elimination-(RFE)-6.2"><span class="toc-item-num">6.2&nbsp;&nbsp;</span>Evaluating model using Recursive Feature Elimination (RFE)</a></span></li></ul></li><li><span><a href="#Conclusion" data-toc-modified-id="Conclusion-7"><span class="toc-item-num">7&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>

### Note:
This notebook is a work in progress and further updates on the modelling and feature selection/engineering part will be rolled out soon.
